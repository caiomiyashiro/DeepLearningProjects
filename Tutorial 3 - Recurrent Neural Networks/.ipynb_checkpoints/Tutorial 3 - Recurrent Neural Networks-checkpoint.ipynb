{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country Classification based on Surname\n",
    "\n",
    "- \n",
    "https://github.com/joosthub/PyTorchNLPBook/blob/master/chapters/chapter_7/7_3_surname_generation/7_3_Model1_Unconditioned_Surname_Generation.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    def __init__(self, surname_list, country_list):\n",
    "        self.surname_ix2char = {}\n",
    "        self.surname_char2ix = {}\n",
    "        self.max_n_chars = np.max(surname_list.apply(len))+2 # len + <INIT> and <END>\n",
    "        self.country_ix2word = {}\n",
    "        self.country_word2ix = {}\n",
    "        self.init = '<init>'\n",
    "        self.end = '<end>'\n",
    "        self.unk = '<unk>'\n",
    "        self.mask = '<mask>'\n",
    "        \n",
    "        self.create_surname_vocabulary(surname_list)\n",
    "        self.create_country_vocabulary(country_list)\n",
    "        \n",
    "    def create_surname_vocabulary(self, surnames):\n",
    "        count_ix = 0\n",
    "        surnames = surnames.str.lower()\n",
    "        for surname in surnames:\n",
    "            for char in surname:\n",
    "                if char not in self.surname_ix2char.values():\n",
    "                    self.surname_ix2char[count_ix] = char\n",
    "                    count_ix += 1\n",
    "\n",
    "        sequence_vocab = [self.init, self.end, self.unk, self.mask]\n",
    "        for special_char in sequence_vocab:\n",
    "            self.surname_ix2char[count_ix] = special_char\n",
    "            count_ix += 1\n",
    "        \n",
    "        self.surname_char2ix = {e: k for k,e in self.surname_ix2char.items()} \n",
    "        \n",
    "    def create_country_vocabulary(self, countries):\n",
    "        count_ix = 0\n",
    "        countries = countries.str.lower()\n",
    "        for country in countries:\n",
    "            if country not in self.country_ix2word.values():\n",
    "                self.country_ix2word[count_ix] = country\n",
    "                count_ix += 1\n",
    "        \n",
    "        self.country_word2ix = {e: k for k,e in self.country_ix2word.items()} \n",
    "        \n",
    "        \n",
    "    def get_surname_char2ix(self, char):\n",
    "        char = char.lower()\n",
    "        if char in self.surname_char2ix.keys():\n",
    "            return self.surname_char2ix[char]\n",
    "        else:\n",
    "            return self.surname_char2ix[self.unk]\n",
    "    \n",
    "    def get_surname_ix2char(self, ix):\n",
    "        return self.surname_ix2char[ix]\n",
    "    \n",
    "    def get_country_word2ix(self, word):\n",
    "        word = word.lower()\n",
    "        return self.country_word2ix[word]\n",
    "    \n",
    "    def get_country_ix2word(self, ix):\n",
    "        return self.country_ix2word[ix]\n",
    "\n",
    "class Vectorizer():\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "        \n",
    "    def vectorize(self, word, vector_type):\n",
    "        word_ixs = []\n",
    "        if(vector_type == 'surname'):\n",
    "            for c in word.lower():\n",
    "                word_ixs.append(self.vocabulary.get_surname_char2ix(c))\n",
    "            return_ixs = [self.vocabulary.get_surname_char2ix(self.vocabulary.init)]\n",
    "            return_ixs.extend(word_ixs)\n",
    "            return_ixs.append(self.vocabulary.get_surname_char2ix(self.vocabulary.end))\n",
    "\n",
    "            size_mask_leftover = self.vocabulary.max_n_chars - len(return_ixs)\n",
    "            return_ixs.extend([self.vocabulary.get_surname_char2ix(self.vocabulary.mask)] * size_mask_leftover)\n",
    "        else:\n",
    "            return_ixs = self.vocabulary.get_country_word2ix(word)\n",
    "        \n",
    "        return return_ixs\n",
    "        \n",
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, csv_file_path):\n",
    "        self.df = pd.read_csv(csv_file_path)\n",
    "        self.vectorizer = Vectorizer(Vocabulary(self.df['surname'], self.df['nationality']))\n",
    "        self.dataset_split = 'train'\n",
    "        self.df_splitted = {}\n",
    "        self.split_df()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.get_dataset_on_type()['size']\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        row = self.get_dataset_on_type()['dataset'].iloc[ix]\n",
    "        x = self.vectorizer.vectorize(row['surname'], 'surname')\n",
    "        y = self.vectorizer.vectorize(row['nationality'], 'nationality')\n",
    "        return {\n",
    "            'X': x,\n",
    "            'y': y\n",
    "        }  \n",
    "    \n",
    "    def set_dataset_split(self, dataset_split_type):\n",
    "        self.dataset_split = dataset_split_type\n",
    "    \n",
    "    def split_df(self):\n",
    "        train_ix = int(d.shape[0] * 0.7)\n",
    "        val_ix = int(d.shape[0] * 0.15)\n",
    "        test_ix = int(d.shape[0] * 0.15)\n",
    "\n",
    "        self.df['split'] = ''\n",
    "        self.df.loc[:train_ix, 'split'] = 'train'\n",
    "        self.df.loc[train_ix:(train_ix + val_ix), 'split'] = 'val'\n",
    "        self.df.loc[(train_ix + val_ix):(train_ix + val_ix + test_ix), 'split'] = 'test'\n",
    "        \n",
    "        split_types = ['train', 'val', 'test']\n",
    "        for st in split_types:\n",
    "             self.df_splitted[st] = {\n",
    "                 'dataset': self.df.loc[self.df['split'] == st],\n",
    "                 'size': self.df.loc[self.df['split'] == st].shape[0]\n",
    "             }\n",
    "    \n",
    "    def get_dataset_on_type(self):\n",
    "        return self.df_splitted[self.dataset_split]\n",
    "        \n",
    "class SequentialModelGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_size, country_class_size, embedding_dim, padding_ix, rnn_hidden_size, batch_first, dropout_p):\n",
    "        \"\"\"\n",
    "        :param char_vocab_size: \n",
    "        :param embedding_dim: \n",
    "        :param padding_ix: \n",
    "        \n",
    "        \"\"\"\n",
    "        super(SequentialModelGRU, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
    "                                embedding_dim=embedding_dim,\n",
    "                                padding_idx=padding_ix)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=embedding_dim, \n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          batch_first=batch_first)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size, \n",
    "                            out_features=country_class_size)\n",
    "        \n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        \n",
    "    def forward(self, x_in, apply_softmax=True):        \n",
    "        # x_in.shape = [19, 64] =  [seq_length, batch_size]\n",
    "        x_in = x_in.permute(1,0)     # shape = [batch_size, seq_length]\n",
    "\n",
    "        x_out = self.emb(x_in)       # shape = [64, 19, 100] [batch_size, seq_length, emb_dim]\n",
    "        \n",
    "        # x_in shape for rnn --> https://discuss.pytorch.org/t/tensor-shape-for-rnn-batch-training/13466        \n",
    "        # h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len\n",
    "        x_out, h_n = self.rnn(x_out) # x_out shape [64, 19, 100] h_n shape [1, 19, 64]\n",
    "        \n",
    "        x_out = x_out.permute(1,0,2)\n",
    "        x_out = self.dropout(x_out[-1])\n",
    "        \n",
    "        x_out = self.fc(x_out) # x_out shape [64,18]  \n",
    "        \n",
    "        if(apply_softmax):\n",
    "            x_out = F.softmax(x_out, dim=1)\n",
    "#             print(x_out)\n",
    "\n",
    "        return x_out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle, drop_last):\n",
    "    \"\"\"\n",
    "    Generate different DataLoaders. Allow for train, val sets generation\n",
    "    \"\"\"\n",
    "    s_dl = DataLoader(dataset=s_df, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in s_dl:\n",
    "        yield data_dict\n",
    "        \n",
    "def get_validation_performance(dataset, model, loss):\n",
    "        dataset.set_dataset_split('val')\n",
    "        n_batches = len(dataset)/batch_size\n",
    "        \n",
    "        s_dl_val = generate_batches(dataset=dataset, batch_size=batch_size, \n",
    "                        shuffle=gen_batches_shuffle, drop_last=gen_batches_drop_last)\n",
    "        moving_loss_val = 0\n",
    "        with torch.no_grad():\n",
    "            model = model.eval()\n",
    "            for batch_ix_val, batch_data_val in enumerate(s_dl_val,0):\n",
    "                X_val = torch.stack(batch_data_val['X'])\n",
    "                y_val = batch_data_val['y']\n",
    "\n",
    "                y_val_pred = model(X_val)\n",
    "\n",
    "                loss_val = loss(y_val_pred, y_val)\n",
    "                moving_loss_val += loss_val.item()\n",
    "\n",
    "\n",
    "        avg_batch_loss_val = moving_loss_val/n_batches\n",
    "        return avg_batch_loss_val        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\n",
      "Training Loss: 2.7104017956782194\n",
      "Validation Loss: 2.5207804526136224\n",
      "--------------------------------\n",
      "epoch 1:\n",
      "Training Loss: 2.5467777716911444\n",
      "Validation Loss: 2.498944313075952\n",
      "--------------------------------\n",
      "epoch 2:\n",
      "Training Loss: 2.5374222104832276\n",
      "Validation Loss: 2.4964449719797863\n",
      "--------------------------------\n",
      "epoch 3:\n",
      "Training Loss: 2.5331581103599676\n",
      "Validation Loss: 2.4930495591907698\n",
      "--------------------------------\n",
      "epoch 4:\n",
      "Training Loss: 2.5292810929023615\n",
      "Validation Loss: 2.4953531668847737\n",
      "--------------------------------\n",
      "epoch 5:\n",
      "Training Loss: 2.515852998878996\n",
      "Validation Loss: 2.4336896083104516\n",
      "--------------------------------\n",
      "epoch 6:\n",
      "Training Loss: 2.4452564170805076\n",
      "Validation Loss: 2.3964455182724604\n",
      "--------------------------------\n",
      "epoch 7:\n",
      "Training Loss: 2.423965328830784\n",
      "Validation Loss: 2.382495584962726\n",
      "--------------------------------\n",
      "epoch 8:\n",
      "Training Loss: 2.414209905317274\n",
      "Validation Loss: 2.379480084290125\n",
      "--------------------------------\n",
      "epoch 9:\n",
      "Training Loss: 2.417360782623291\n",
      "Validation Loss: 2.3887051476372614\n",
      "--------------------------------\n",
      "epoch 10:\n",
      "Training Loss: 2.4158803669072815\n",
      "Validation Loss: 2.380301502160615\n",
      "--------------------------------\n",
      "epoch 11:\n",
      "Training Loss: 2.4144068309816262\n",
      "Validation Loss: 2.3796488410715777\n",
      "--------------------------------\n",
      "epoch 12:\n",
      "Training Loss: 2.4119050947286316\n",
      "Validation Loss: 2.375377878392908\n",
      "--------------------------------\n",
      "epoch 13:\n",
      "Training Loss: 2.414496553146233\n",
      "Validation Loss: 2.383032191067518\n",
      "--------------------------------\n",
      "epoch 14:\n",
      "Training Loss: 2.411592140036114\n",
      "Validation Loss: 2.3850863003050407\n",
      "--------------------------------\n",
      "epoch 15:\n",
      "Training Loss: 2.406981116634304\n",
      "Validation Loss: 2.38649880444562\n",
      "--------------------------------\n",
      "epoch 16:\n",
      "Training Loss: 2.4064598325955666\n",
      "Validation Loss: 2.372085893233762\n",
      "--------------------------------\n",
      "epoch 17:\n",
      "Training Loss: 2.400919558638233\n",
      "Validation Loss: 2.3724099796471196\n",
      "--------------------------------\n",
      "epoch 18:\n",
      "Training Loss: 2.409929134077945\n",
      "Validation Loss: 2.371954849147044\n",
      "--------------------------------\n",
      "epoch 19:\n",
      "Training Loss: 2.3972008773836038\n",
      "Validation Loss: 2.374212059311673\n",
      "--------------------------------\n",
      "epoch 20:\n",
      "Training Loss: 2.3985129776647534\n",
      "Validation Loss: 2.375796387683426\n",
      "--------------------------------\n",
      "epoch 21:\n",
      "Training Loss: 2.3990500761290727\n",
      "Validation Loss: 2.3775263070046866\n",
      "--------------------------------\n",
      "epoch 22:\n",
      "Training Loss: 2.3990583743079235\n",
      "Validation Loss: 2.375936361248014\n",
      "--------------------------------\n",
      "epoch 23:\n",
      "Training Loss: 2.397424857495195\n",
      "Validation Loss: 2.3658843661483737\n",
      "--------------------------------\n",
      "epoch 24:\n",
      "Training Loss: 2.3787924596818826\n",
      "Validation Loss: 2.3508943509679168\n",
      "--------------------------------\n",
      "epoch 25:\n",
      "Training Loss: 2.364832924584211\n",
      "Validation Loss: 2.3465097326905204\n",
      "--------------------------------\n",
      "epoch 26:\n",
      "Training Loss: 2.3584273831319003\n",
      "Validation Loss: 2.3340991322170552\n",
      "--------------------------------\n",
      "epoch 27:\n",
      "Training Loss: 2.350836097183874\n",
      "Validation Loss: 2.3318871904592045\n",
      "--------------------------------\n",
      "epoch 28:\n",
      "Training Loss: 2.345623491174084\n",
      "Validation Loss: 2.3446443751137696\n",
      "--------------------------------\n",
      "epoch 29:\n",
      "Training Loss: 2.346500146186958\n",
      "Validation Loss: 2.3373021142875055\n",
      "--------------------------------\n",
      "epoch 30:\n",
      "Training Loss: 2.3519729052559804\n",
      "Validation Loss: 2.333769042635225\n",
      "--------------------------------\n",
      "epoch 31:\n",
      "Training Loss: 2.3429539648153015\n",
      "Validation Loss: 2.3368664423335157\n",
      "--------------------------------\n",
      "epoch 32:\n",
      "Training Loss: 2.3437050540568465\n",
      "Validation Loss: 2.3359455487622447\n",
      "--------------------------------\n",
      "epoch 33:\n",
      "Training Loss: 2.339151810791533\n",
      "Validation Loss: 2.3253284654403066\n",
      "--------------------------------\n",
      "epoch 34:\n",
      "Training Loss: 2.3277887368606307\n",
      "Validation Loss: 2.312423384544266\n",
      "--------------------------------\n",
      "epoch 35:\n",
      "Training Loss: 2.319093849699376\n",
      "Validation Loss: 2.3133419634142136\n",
      "--------------------------------\n",
      "epoch 36:\n",
      "Training Loss: 2.3132119986970547\n",
      "Validation Loss: 2.31098827913304\n",
      "--------------------------------\n",
      "epoch 37:\n",
      "Training Loss: 2.3055250907348372\n",
      "Validation Loss: 2.3069994089164805\n",
      "--------------------------------\n",
      "epoch 38:\n",
      "Training Loss: 2.3037824246842984\n",
      "Validation Loss: 2.306504984950035\n",
      "--------------------------------\n",
      "epoch 39:\n",
      "Training Loss: 2.3023085068848173\n",
      "Validation Loss: 2.302544418073236\n",
      "--------------------------------\n",
      "epoch 40:\n",
      "Training Loss: 2.294915716526872\n",
      "Validation Loss: 2.295612612129633\n",
      "--------------------------------\n",
      "epoch 41:\n",
      "Training Loss: 2.293164382546635\n",
      "Validation Loss: 2.308690128864921\n",
      "--------------------------------\n",
      "epoch 42:\n",
      "Training Loss: 2.294663920240887\n",
      "Validation Loss: 2.298512161320893\n",
      "--------------------------------\n",
      "epoch 43:\n",
      "Training Loss: 2.29103364378719\n",
      "Validation Loss: 2.302526267427492\n",
      "--------------------------------\n",
      "epoch 44:\n",
      "Training Loss: 2.291185564913992\n",
      "Validation Loss: 2.2993202555447403\n",
      "--------------------------------\n",
      "epoch 45:\n",
      "Training Loss: 2.290855258198108\n",
      "Validation Loss: 2.2972578566654858\n",
      "--------------------------------\n",
      "epoch 46:\n",
      "Training Loss: 2.290261420152955\n",
      "Validation Loss: 2.294044985362958\n",
      "--------------------------------\n",
      "epoch 47:\n",
      "Training Loss: 2.2886508723436774\n",
      "Validation Loss: 2.2923552910342098\n",
      "--------------------------------\n",
      "epoch 48:\n",
      "Training Loss: 2.2885843454781227\n",
      "Validation Loss: 2.2986511939499836\n",
      "--------------------------------\n",
      "epoch 49:\n",
      "Training Loss: 2.3015164884470276\n",
      "Validation Loss: 2.312191293394486\n",
      "--------------------------------\n",
      "epoch 50:\n",
      "Training Loss: 2.294870647333436\n",
      "Validation Loss: 2.2908469996599985\n",
      "--------------------------------\n",
      "epoch 51:\n",
      "Training Loss: 2.2850510609351984\n",
      "Validation Loss: 2.2945526671105614\n",
      "--------------------------------\n",
      "epoch 52:\n",
      "Training Loss: 2.2888464523574052\n",
      "Validation Loss: 2.2921051527415903\n",
      "--------------------------------\n",
      "epoch 53:\n",
      "Training Loss: 2.284067335775343\n",
      "Validation Loss: 2.292092346609471\n",
      "--------------------------------\n",
      "epoch 54:\n",
      "Training Loss: 2.283318966121997\n",
      "Validation Loss: 2.2919943199493558\n",
      "--------------------------------\n",
      "epoch 55:\n",
      "Training Loss: 2.280764442379192\n",
      "Validation Loss: 2.28688824878871\n",
      "--------------------------------\n",
      "epoch 56:\n",
      "Training Loss: 2.285883115509809\n",
      "Validation Loss: 2.30253587437892\n",
      "--------------------------------\n",
      "epoch 57:\n",
      "Training Loss: 2.2886046858157143\n",
      "Validation Loss: 2.296264021774459\n",
      "--------------------------------\n",
      "epoch 58:\n",
      "Training Loss: 2.285441097566637\n",
      "Validation Loss: 2.293736876034346\n",
      "--------------------------------\n",
      "epoch 59:\n",
      "Training Loss: 2.2807089272191967\n",
      "Validation Loss: 2.2885737841825597\n",
      "--------------------------------\n",
      "epoch 60:\n",
      "Training Loss: 2.2800705392481917\n",
      "Validation Loss: 2.2919861808573034\n",
      "--------------------------------\n",
      "epoch 61:\n",
      "Training Loss: 2.2835629046973533\n",
      "Validation Loss: 2.313829913744437\n",
      "--------------------------------\n",
      "epoch 62:\n",
      "Training Loss: 2.2906095436063865\n",
      "Validation Loss: 2.2944815417963156\n",
      "--------------------------------\n",
      "epoch 63:\n",
      "Training Loss: 2.2800033294548423\n",
      "Validation Loss: 2.3010148239193646\n",
      "--------------------------------\n",
      "epoch 64:\n",
      "Training Loss: 2.2851756326222823\n",
      "Validation Loss: 2.287865193887559\n",
      "--------------------------------\n",
      "epoch 65:\n",
      "Training Loss: 2.280126038244215\n",
      "Validation Loss: 2.2907916256059786\n",
      "--------------------------------\n",
      "epoch 66:\n",
      "Training Loss: 2.2814567937689314\n",
      "Validation Loss: 2.2959830207975256\n",
      "--------------------------------\n",
      "epoch 67:\n",
      "Training Loss: 2.2795624147027227\n",
      "Validation Loss: 2.287205541647776\n",
      "--------------------------------\n",
      "epoch 68:\n",
      "Training Loss: 2.2794319310430753\n",
      "Validation Loss: 2.286197592724289\n",
      "--------------------------------\n",
      "epoch 69:\n",
      "Training Loss: 2.278545296798318\n",
      "Validation Loss: 2.2868815305092585\n",
      "--------------------------------\n",
      "epoch 70:\n",
      "Training Loss: 2.2767827328989063\n",
      "Validation Loss: 2.286342609703143\n",
      "--------------------------------\n",
      "epoch 71:\n",
      "Training Loss: 2.276947866051884\n",
      "Validation Loss: 2.2854335342817187\n",
      "--------------------------------\n",
      "epoch 72:\n",
      "Training Loss: 2.2769238787182307\n",
      "Validation Loss: 2.287268151495771\n",
      "--------------------------------\n",
      "epoch 73:\n",
      "Training Loss: 2.2758947489625316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2874867684492286\n",
      "--------------------------------\n",
      "epoch 74:\n",
      "Training Loss: 2.2742357375258107\n",
      "Validation Loss: 2.2854693745148134\n",
      "--------------------------------\n",
      "epoch 75:\n",
      "Training Loss: 2.2754199565467186\n",
      "Validation Loss: 2.283042033802906\n",
      "--------------------------------\n",
      "epoch 76:\n",
      "Training Loss: 2.2731854168035217\n",
      "Validation Loss: 2.2834035224022835\n",
      "--------------------------------\n",
      "epoch 77:\n",
      "Training Loss: 2.2735605947041915\n",
      "Validation Loss: 2.285668391781861\n",
      "--------------------------------\n",
      "epoch 78:\n",
      "Training Loss: 2.2741019422725097\n",
      "Validation Loss: 2.2884003039049103\n",
      "--------------------------------\n",
      "epoch 79:\n",
      "Training Loss: 2.274435481782687\n",
      "Validation Loss: 2.2860345003745306\n",
      "--------------------------------\n",
      "epoch 80:\n",
      "Training Loss: 2.2755593263496787\n",
      "Validation Loss: 2.284219021788495\n",
      "--------------------------------\n",
      "epoch 81:\n",
      "Training Loss: 2.2781182244672613\n",
      "Validation Loss: 2.291351143490132\n",
      "--------------------------------\n",
      "epoch 82:\n",
      "Training Loss: 2.279324006226103\n",
      "Validation Loss: 2.28641332100577\n",
      "--------------------------------\n",
      "epoch 83:\n",
      "Training Loss: 2.2761206040948125\n",
      "Validation Loss: 2.2918244340453064\n",
      "--------------------------------\n",
      "epoch 84:\n",
      "Training Loss: 2.2834018327422063\n",
      "Validation Loss: 2.298759410941536\n",
      "--------------------------------\n",
      "epoch 85:\n",
      "Training Loss: 2.281605294195272\n",
      "Validation Loss: 2.2917764557442957\n",
      "--------------------------------\n",
      "epoch 86:\n",
      "Training Loss: 2.275517384884721\n",
      "Validation Loss: 2.283647572842235\n",
      "--------------------------------\n",
      "epoch 87:\n",
      "Training Loss: 2.2753330650976147\n",
      "Validation Loss: 2.285679174902661\n",
      "--------------------------------\n",
      "epoch 88:\n",
      "Training Loss: 2.277575424162008\n",
      "Validation Loss: 2.2996501569394714\n",
      "--------------------------------\n",
      "epoch 89:\n",
      "Training Loss: 2.27898129366212\n",
      "Validation Loss: 2.290046009499157\n",
      "--------------------------------\n",
      "epoch 90:\n",
      "Training Loss: 2.2794985508514665\n",
      "Validation Loss: 2.290630829138834\n",
      "--------------------------------\n",
      "epoch 91:\n",
      "Training Loss: 2.276064569667234\n",
      "Validation Loss: 2.2819714373940037\n",
      "--------------------------------\n",
      "epoch 92:\n",
      "Training Loss: 2.2768890170727745\n",
      "Validation Loss: 2.2850746708923784\n",
      "--------------------------------\n",
      "epoch 93:\n",
      "Training Loss: 2.2766492710275164\n",
      "Validation Loss: 2.2789168878982626\n",
      "--------------------------------\n",
      "epoch 94:\n",
      "Training Loss: 2.276211595131179\n",
      "Validation Loss: 2.2878443427569373\n",
      "--------------------------------\n",
      "epoch 95:\n",
      "Training Loss: 2.276080565937495\n",
      "Validation Loss: 2.2838378205904473\n",
      "--------------------------------\n",
      "epoch 96:\n",
      "Training Loss: 2.2732130511332365\n",
      "Validation Loss: 2.287642634414367\n",
      "--------------------------------\n",
      "epoch 97:\n",
      "Training Loss: 2.273644604925382\n",
      "Validation Loss: 2.2880721432408495\n",
      "--------------------------------\n",
      "epoch 98:\n",
      "Training Loss: 2.2770869428828613\n",
      "Validation Loss: 2.28958595790643\n",
      "--------------------------------\n",
      "epoch 99:\n",
      "Training Loss: 2.274507690284212\n",
      "Validation Loss: 2.2857457366797442\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_size = 65\n",
    "lr = 1e-3\n",
    "n_epochs = 100\n",
    "char_vocab_size = len(s_df.vectorizer.vocabulary.surname_char2ix)\n",
    "country_class_size = len(s_df.vectorizer.vocabulary.country_word2ix)\n",
    "embedding_dim = 100\n",
    "padding_ix = s_df.vectorizer.vocabulary.get_surname_char2ix('<mask>')\n",
    "rnn_hidden_size = 64\n",
    "batch_first = True\n",
    "dropout_p = .3\n",
    "gen_batches_shuffle = True\n",
    "gen_batches_drop_last = True\n",
    "\n",
    "n_batches = int(len(s_df)/batch_size)\n",
    "\n",
    "s_df = SurnameDataset('data/surnames.csv')\n",
    "\n",
    "\n",
    "rnn = SequentialModelGRU(char_vocab_size=char_vocab_size, country_class_size=country_class_size, \n",
    "                         embedding_dim=embedding_dim, padding_ix=padding_ix, rnn_hidden_size=rnn_hidden_size,\n",
    "                         batch_first=batch_first, dropout_p=dropout_p)\n",
    "\n",
    "loss_ce = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "avg_batch_loss_train_all = []\n",
    "avg_batch_loss_val_all = []\n",
    "\n",
    "for ep in range(n_epochs):\n",
    "    moving_loss_train = 0\n",
    "    s_df.set_dataset_split('train')\n",
    "    s_dl = generate_batches(dataset=s_df, batch_size=batch_size, \n",
    "                            shuffle=gen_batches_shuffle, drop_last=gen_batches_drop_last)\n",
    "    rnn = rnn.train()\n",
    "    for batch_ix, batch_data in enumerate(s_dl, 0):\n",
    "        X = torch.stack(batch_data['X'])\n",
    "        y = batch_data['y']\n",
    "        \n",
    "        rnn.zero_grad()\n",
    "        \n",
    "        y_pred = rnn(X)\n",
    "        \n",
    "        loss = loss_ce(y_pred, y) # size of y_pred: https://github.com/pytorch/pytorch/issues/5554\n",
    "        moving_loss_train += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimiser.step()\n",
    "        \n",
    "    avg_batch_loss_train = moving_loss_train/n_batches\n",
    "    avg_batch_loss_train_all.append(avg_batch_loss_train)\n",
    "    print(f'epoch {ep}:')\n",
    "    print(f'Training Loss: {avg_batch_loss_train}')\n",
    "    \n",
    "    # validation performance\n",
    "    avg_batch_loss_val = get_validation_performance(dataset=s_df,\n",
    "                                                    model=rnn,\n",
    "                                                    loss=loss_ce)\n",
    "    avg_batch_loss_val_all.append(avg_batch_loss_val)\n",
    "    \n",
    "    print(f'Validation Loss: {avg_batch_loss_val}')\n",
    "    print(f'--------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surname: McMahan, probability 0.9999935626983643 - Class: english\n",
      "Surname: Nakamoto, probability 0.9999966621398926 - Class: japanese\n",
      "Surname: Wan, probability 0.9999995231628418 - Class: english\n",
      "Surname: Cho, probability 0.9999802112579346 - Class: japanese\n"
     ]
    }
   ],
   "source": [
    "def get_prediction(surname, classifier, vectorizer): \n",
    "    classifier = classifier.eval()\n",
    "    vectorized_surname = torch.Tensor(vectorizer.vectorize(surname, 'surname')).type(torch.long)\n",
    "    vectorized_surname = vectorized_surname.unsqueeze(0).permute(1,0)\n",
    "\n",
    "    prediction = classifier(vectorized_surname)\n",
    "\n",
    "    prob, ix = prediction.max(dim=1)\n",
    "    prob = prob.squeeze(0).item()\n",
    "    ix = ix.squeeze(0).item()\n",
    "    \n",
    "    country = vectorizer.vocabulary.get_country_ix2word(ix)\n",
    "\n",
    "    return prob, country\n",
    "\n",
    "surnames = ['McMahan', 'Nakamoto', 'Wan', 'Cho']\n",
    "for surname in surnames:\n",
    "    prob, country = get_prediction(surname=surname, classifier=rnn, vectorizer=s_df.vectorizer)\n",
    "    print(f'Surname: {surname}, probability {prob} - Class: {country}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "- Early Stopping\n",
    "- Examples from test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surname Generation based on Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
